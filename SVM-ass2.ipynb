{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6db4668-db78-45b3-8e83-2dec231f7ce0",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2224b-6a67-4a68-975e-8837bb08ea73",
   "metadata": {},
   "source": [
    "Relationship between Polynomial Functions and Polynomial Kernel Functions:\n",
    "1. Representation: Polynomial functions in SVMs serve as the basis for constructing polynomial kernel functions. The polynomial kernel \n",
    "\n",
    "        K(x,x′)=(x⊤x′+c)d\n",
    "\n",
    "defines a similarity measure that corresponds to the dot product in a higher-dimensional space where features are polynomial combinations of the original features.\n",
    "\n",
    "2. Computational Efficiency: Kernel functions, including polynomial kernels, leverage the kernel trick, which avoids explicitly computing the transformation of features into the higher-dimensional space. This approach is computationally efficient compared to explicitly transforming features, especially when dealing with high-dimensional data.\n",
    "\n",
    "3. Flexibility: SVMs with polynomial kernels offer flexibility in capturing non-linear relationships between features. By adjusting the degree d of the polynomial kernel, the model can capture varying degrees of non-linearity in the data.\n",
    "\n",
    " polynomial functions can be used as kernel functions in SVMs and other kernel-based machine learning algorithms to enable non-linear modeling capabilities by implicitly transforming input features into higher-dimensional spaces defined by polynomial expansions. This relationship allows SVMs to effectively handle complex patterns and relationships in the data without the need for explicit feature mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e1072-d2df-43a7-baea-735c31aa2314",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386bbc2e-588a-41d9-ae8a-113e647bfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1ca3a0-513e-4034-a523-79e5af63e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1547db3b-6631-47e0-8bf7-1d7f0d6203aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df26081e-097f-480b-8108-86b6ecf63d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69529250-df3e-40f1-92de-33b71689dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.24,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279c41e6-76d7-450f-b549-95257b4de563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15686, 8), (15686,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edb2e0-7ead-4781-8ffc-d5120b631a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]..........................................................................................................................................................................................................................................................................................................................................................................*...................................................................................................................................................*....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_poly = SVR(kernel=\"poly\",degree= 3, verbose= True)\n",
    "svr_poly.fit(X_train,Y_train)\n",
    "Y_pred = svr_poly.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7868c7-65ff-4e4b-ad77-886efe75bc69",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6138d-7619-4679-a0e7-ffe5fceb8dc3",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon (ε) plays a crucial role in determining the margin within which no penalty is incurred for errors. It influences the number of support vectors in the following way:\n",
    "\n",
    "## Definition of Epsilon: \n",
    "Epsilon (ε) defines a tube around the predicted regression function within which errors are acceptable and do not contribute to the loss function. Specifically, SVR aims to minimize errors outside this tube, penalizing deviations beyond it.\n",
    "\n",
    "## Impact on Support Vectors:\n",
    "\n",
    "1. Smaller Epsilon: When epsilon is small, the SVR model becomes more sensitive to errors, and thus, the tube around the regression line becomes narrower. As a result, fewer data points are allowed within this narrow margin, leading to fewer support vectors.\n",
    "\n",
    "2. Larger Epsilon: Conversely, a larger epsilon creates a wider tube around the regression line where errors are tolerated. This wider margin allows more data points to fall within the margin without penalty, potentially increasing the number of support vectors.\n",
    "\n",
    "3. Relationship: The number of support vectors typically increases as epsilon increases because a wider margin allows the model to capture more data points within the permissible error range. These support vectors are the critical points that influence the construction of the regression function in SVR.\n",
    "\n",
    "4. Balancing Act: However, while a larger epsilon might lead to more support vectors and potentially a more flexible model, it could also result in a model that generalizes less effectively to unseen data, as it may overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c50f3-71da-4edb-a1d5-81dc1a088b63",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137e7f4-49ee-45aa-a586-cf1b41cdeb4a",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) performance is influenced by several key parameters: the choice of kernel function, C parameter, epsilon parameter (ε), and gamma parameter (γ). Let's delve into each parameter, how it works, and when you might adjust its value:\n",
    "\n",
    "1. Kernel Function:\n",
    "Definition: The kernel function determines the type of decision boundary used by SVR. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "Impact: The kernel function controls the mapping of input features into a higher-dimensional space where SVR finds a linear relationship between the features and the target variable.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "Linear Kernel: Suitable when the relationship between features and target variable is approximately linear.\n",
    "\n",
    "Polynomial Kernel: Useful for capturing non-linear relationships; adjust the degree parameter to control the complexity of the polynomial.\n",
    "\n",
    "RBF Kernel: Generally effective for capturing complex non-linear relationships; adjust the gamma parameter to control the influence of each training example.\n",
    "\n",
    "Adjustment: Choose the kernel function based on the complexity and non-linearity of the relationship between features and the target variable. Increase complexity (e.g., use RBF instead of linear) when the relationship is non-linear and requires more flexible decision boundaries.\n",
    "\n",
    "2. C Parameter:\n",
    "Definition: The C parameter controls the trade-off between achieving a low training error and a low model complexity (smooth decision boundary).\n",
    "\n",
    "Impact: A larger C encourages the model to fit the training data more closely, potentially leading to overfitting. A smaller C allows more flexibility in the margin and can lead to underfitting if too small.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Large C: Use when you want to minimize training error and are less concerned about overfitting. Suitable when the data is well-behaved and not noisy.\n",
    "\n",
    "Small C: Use when you prioritize a wider margin and regularization to prevent overfitting. Suitable when the data is noisy or when there are outliers.\n",
    "\n",
    "Adjustment: Tune C using cross-validation to find a balance between fitting the training data well and generalizing to unseen data.\n",
    "\n",
    "3. Epsilon Parameter (ε):\n",
    "Definition: Epsilon defines a tube around the regression line within which no penalty is associated with errors. SVR aims to fit as many instances as possible within this tube.\n",
    "\n",
    "Impact: A smaller epsilon makes the SVR model more sensitive to errors, potentially resulting in fewer support vectors and a narrower margin. A larger epsilon allows more instances to be inside the tube, potentially increasing the number of support vectors.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Small ε: Use when you want to minimize deviations from the predicted function; suitable when precise predictions are crucial.\n",
    "\n",
    "Large ε: Use when you can tolerate larger errors and want to ensure a simpler model with more data points fitting within the tube.\n",
    "\n",
    "Adjustment: Tune epsilon based on the acceptable margin of error for your application and the amount of noise in the data.\n",
    "\n",
    "4. Gamma Parameter (γ):\n",
    "Definition: Gamma parameter defines how far the influence of a single training example reaches. It affects the shape of the decision boundary.\n",
    "\n",
    "Impact: A smaller gamma makes the decision boundary more linear, while a larger gamma makes it more non-linear and can lead to overfitting.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Small γ: Use when the decision boundary is expected to be smooth (e.g., in linear or simpler relationships).\n",
    "\n",
    "Large γ: Use when the decision boundary is expected to be complex (e.g., in non-linear or more complex relationships).\n",
    "\n",
    "Adjustment: Tune gamma to control the influence of individual training examples; higher values make the model more sensitive to variations in individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb2c48-d15c-4a61-868a-b9b8622dbf80",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "Import the necessary libraries and load the dataset\n",
    " \n",
    "Split the dataset into training and testing sets\n",
    "\n",
    "Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "Use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score)\n",
    "\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "\n",
    "Train the tuned classifier on the entire dataset\n",
    "\n",
    "Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f15956a-16e5-419d-b695-287db6ce8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       1.00      1.00      1.00        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       1.00      0.97      0.99        34\n",
      "           4       0.96      1.00      0.98        46\n",
      "           5       0.96      0.98      0.97        47\n",
      "           6       0.97      1.00      0.99        35\n",
      "           7       1.00      0.94      0.97        34\n",
      "           8       0.97      0.97      0.97        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "Best Parameters: {'C': 100, 'gamma': 0.01}\n",
      "Best Cross-validation Accuracy: 0.9812161246612467\n",
      "Trained classifier saved to 'svm_classifier.pkl'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (digits dataset from Scikit-learn for demonstration)\n",
    "digits = load_digits()\n",
    "X = digits.data  # features\n",
    "y = digits.target  # target variable\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file using joblib\n",
    "joblib.dump(best_svc, 'svm_classifier.pkl')\n",
    "print(\"Trained classifier saved to 'svm_classifier.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f84a9-9aeb-44e4-b617-c3df82f05d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
